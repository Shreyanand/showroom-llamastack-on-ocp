= Getting Started with Llama Stack

== Exploring the lab
The lab is designed to be interactive and hands-on, allowing you to explore the capabilities of LlamaStack and OpenShift AI in a practical setting. The lab environment is pre-configured with all necessary components, so you can focus on learning and experimentation without worrying about setup or configuration issues.

=== Exploring the nodes
The lab environment consists of multiple nodes but the important ones to observe are the GPU nodes. In the OpenShift web console perform the following steps to view the nodes:

1. In the OpenShift web console, navigate to **Compute** > **Nodes**.
2. Click Roles to filter the nodes by role.
3. Identify the nodes with the role **worker, worker-gpu** are Ready.

image::nodes.png[OpenShift Nodes, width=100%]

=== Accessing OpenShift AI
The OpenShift AI console is accessible via the OpenShift web console. You can log in using the same credentials used to login to the OpenShift environment to access the OpenShift AI.

Locate the OpenShift Route for the OpenShift AI console by performing the following steps:

1. In the OpenShift web console, navigate to **Networking** > **Routes**.
2. Filter the list of routes by specifying the Name **rhods**
3. Click on the URL link to access the OpenShift AI console.
4. Log in using the same credentials used to log in to the OpenShift environment.

=== Model serving
Two models are available for serving in the lab environment:

* Granite 3.2 8B
* Llama 3.2 3B

The image below shows the model serving status in the OpenShift AI console.

image::model-deploys.png[Model Deployments, width=100%]

These models and their status can be viewed in the OpenShift AI console. Perform the following steps to view the models:

1. In the OpenShift AI console, navigate to **Models** > **Model Deployments**.
2. Look for a green check mark next to the model name. This indicates that the model is successfully deployed and ready for use.

The models have already been defined within the Llama Stack configuration so that they are ready to be used by the Workbench.

=== Accessing the Workbench
The Workbench is a collaborative environment that allows you to interact with the Llama Stack instance and interact with various AI tools including MCP.
To access the Workbench, perform the following steps:

1. In the OpenShift AI console, navigate to **Data science projects**
2. Click on the **llama-serve** project
3. Click on the **Workbench** tab
4. Click on the blue **Lab** button which will open a new tab in your browser to the notebook server

You are now ready to start using the Workbench and explore the capabilities of LlamaStack and OpenShift AI. The Workbench provides a Jupyter-like interface where you can create and run notebooks, access data, and interact with the deployed models.

== OpenShift AI

OpenShift AI (previously known as Red Hat OpenShift Data Science) is a comprehensive platform designed to streamline the deployment and management of AI/ML workloads on Red Hat OpenShift. It provides data scientists and ML engineers with a robust environment for developing, training, and deploying machine learning models at scale. OpenShift AI integrates various components of the AI/ML lifecycle, including JupyterHub for collaborative notebook experiences, model serving capabilities, and a curated selection of popular frameworks and tools. The platform emphasizes enterprise-grade security, compliance, and governance while offering the flexibility to run AI workloads across hybrid cloud environments. By leveraging Kubernetes orchestration through OpenShift, it ensures consistent deployment experiences and enables organizations to operationalize AI initiatives with greater efficiency and reliability.

== LlamaStack Project

LlamaStack is an open-source project originated by Meta that has revolutionized the field of large language models (LLMs). The project centers around the Llama family of models, which offer powerful natural language processing capabilities while being more efficient and accessible than many commercial alternatives. LlamaStack encompasses not just the models themselves but also a growing ecosystem of tools and frameworks for fine-tuning, deployment, and integration. What makes LlamaStack particularly significant is its approach to democratizing access to advanced AI capabilities through permissive licensing that allows for commercial use. The project has garnered substantial community support, leading to numerous implementations, adaptations, and extensions that expand its utility across diverse applications from conversational agents to content generation and knowledge extraction.

== vLLM

vLLM is a high-performance library for LLM inference and serving that dramatically improves the efficiency of deploying large language models. At its core, vLLM implements PagedAttention, an innovative technique that optimizes memory management by storing KV (Key-Value) cache in non-contiguous memory blocks, similar to virtual memory in operating systems. This approach significantly increases throughput and enables more efficient batch processing of requests. vLLM supports various models including the Llama family, Mistral, and many others, while providing APIs that are compatible with popular frameworks like HuggingFace Transformers. The library excels at continuous batching, effectively handling multiple concurrent requests to maximize GPU utilization. For production deployments, vLLM offers a FastAPI server with OpenAI-compatible endpoints, making it straightforward to integrate into existing applications and workflows that currently use commercial LLM services.
